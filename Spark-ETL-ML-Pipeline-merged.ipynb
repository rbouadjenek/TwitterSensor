{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 36pt; font-family: georgia, palatino, serif; color: #800000;\">Learning Topical Social Sensors</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>How useful is twitter to you in terms of finding the right information?</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/search.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"><span style=\"text-decoration: underline;\"><span style=\"font-size: 20pt;\"><em><strong>We can do better than this!</strong> </em></span></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><strong>In this project, we are aiming to train a classifier to identify targeted information on Twitter with high precision. </strong></p>\n",
    "<p style=\"text-align: left;\"><strong>For example, if you are interested in:</strong></p>\n",
    "<p style=\"text-align: left;\"><em><strong>&bull; Global social issues</strong></em><br /><em><strong>&bull; Politics in the Pacific Northwest</strong></em><br /><em><strong>&bull; Public transit in New York City</strong></em></p>\n",
    "<p style=\"text-align: left;\"><strong>The classifier would serve as a \"sensor\" to identify topical tweets based on your tailored interests!</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Challenges</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><strong>(1) &nbsp;Billions of potential features, thousands of useful ones (Hashtags, users, mentions, terms, locations)</strong></p>\n",
    "<p style=\"text-align: left;\"><strong>(2) &nbsp;Need a lot of labeled data to learn feature weights well</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Solution</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 12pt;\"><strong>(1) Careful feature engineering and feature selection using Apache Spark.</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>We performed feature selection and transformation with Apache Spark on a standalone server with eight 1TB Hard disks, two 20 core CPU (40 threads) and 256GB RAM. </strong></span></p>\n",
    "<p><span style=\"font-size: 12pt;\"><strong>(2)</strong> <strong>Hashtags!</strong>&nbsp;</span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>Hashtags&nbsp;originated on IRC chat, were&nbsp;adopted later (and perhaps most famously) on Twitter, and&nbsp;now appear on other social media platforms such as Instagram,&nbsp;Tumblr, and Facebook. They usually serve as surogates for topics. Therefore, for each topic,&nbsp;we leverage a (small)&nbsp;set of user-curated topical hashtags to efficiently provide&nbsp;a large number of supervised topic labels for social media&nbsp;content.&nbsp;</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>We used 4 independent annotators to query the Twitter search API to identify candidate hashtags for each topic. A&nbsp;hashtag is assigned to a topic set if 3 out of 4 annotators agrees on the assignment.</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\"><strong>For example, for the topic, \"Natural Disaster\", the set of hashtags are [\"sandy\", \"drought\", \"storm\", \"hurricane\", \"tornado\" .... etc]. If a tweet contains one or more of the pre-determined hashtags, we say it is \"topical\" for a particular toic, and it is labeled 1 (0 otherwise). We will revisit this in the feature selection section</strong></span></p>\n",
    "<p><span style=\"font-size: 10pt;\">&nbsp;</span></p>\n",
    "<p><span style=\"font-size: 18pt; color: #ff0000;\"><strong>Catch!</strong></span></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">Hashtag is part of our feature, wouldn't the classifier simply learn to remember the hashtag?</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">To ensure maximum generality, we remove training hashtags from the validation and test set to ensure the classifier making prediction on the learnt feature and not just remembering hashtags. This would be further illlustrated in the Train-Validation split section later.</span></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Now we have labeled data, what features could be useful for predciting topicality?</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/twt.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left;\"><span style=\"font-size: 18pt; color: #000080;\"><strong>Why might these tweet features be useful?</strong> </span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Users: who tweets on the topic?</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>Tweets from the weather channel might be a good indicator for Natural Disasters</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Hashtags: What hashtags co-occur with the topic?</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>#teaparty could imply LBGT rights</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Mentions:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>@Redcross might be releavant to Natural Disaster</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Locations:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong>Philippines where a lot of natural disaster happend in the last few years is a descent guess for releavant topics</strong></em></span></span></p>\n",
    "<p style=\"text-align: left;\"><br /><span style=\"font-size: 10pt;\"><strong>&bull; Terms:</strong></span><br /><span style=\"font-size: 10pt;\"> <em><strong>-</strong></em> <span style=\"text-decoration: underline;\"><em><strong> Word features are strong indicators of a particular topic</strong></em></span></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Implementation</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 10pt;\">The original Twitter data were collected over 2 years, which contains over 2TB compressed data. It consists of hundreds of millions lines of tweets.</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">How do we go from the raw data to an efficient classifier?</span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">The following three-step processes serves an end-to-end pipeline to perform ETL and ML training.</span></strong></p>\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 12pt;\">Starting the Spark app.</span></strong></span></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">Note that spark context must be lanuched prior to running this note book.</span></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"list-style-type: disc;\">\n",
    "<li><strong><span style=\"font-size: 10pt;\">Spark dir: \"/usr/local/share/spark-2.0.1-bin-hadoop2.7\"</span></strong></li>\n",
    "<li><strong><span style=\"font-size: 10pt;\">Spark config dir: \"/usr/local/share/spark-2.0.1-bin-hadoop2.7/conf\"</span></strong></li>\n",
    "</ul>\n",
    "<p><strong><span style=\"font-size: 10pt;\">The spark configuration is set through the spark-env.sh file. You will need to edit the # of executor, memory and cpu depends on different task. Once these are configured, you can use the command \" start-master.sh\" to start the master node, and then \"start-slave.sh spark://d3m1:7077\" to start the slave node. Note that you should specify the master node url so the slave node knows which maste to communicate to. When you are done, run \"stop-master.sh\" and \"stop-slave.sh\" to stop the corresponding service.<br /> </span></strong></p>\n",
    "<p><strong><span style=\"font-size: 10pt;\">When the service has been started, run the interactive shell with ipython notebook: <br /></span></strong></p>\n",
    "<ul>\n",
    "<li><strong><span style=\"font-size: 10pt;\">\"PYSPARK_DRIVER_PYTHON=\"jupyter\" PYSPARK_DRIVER_PYTHON_OPTS=\"notebook pyspark --master spark://d3m1:7077\"&nbsp;</span></strong></li>\n",
    "</ul>\n",
    "<p><strong><span style=\"font-size: 10pt;\">or if you want to ssh to the server, add the following option \"--no-browser --port=8889\"</span></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step One: Pre-Processing</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Each valid tweet crawled from the server is a json object with over 100 attributes. An example could be find as following:</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 10pt;\"><strong>Sample Tweet</strong></span></p>\n",
    "<p><span style=\"font-size: 8pt;\"><strong>{</strong>\"created_at\":\"Thu Jan 31 12:58:06 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"id\":296965581582786560,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"296965581582786560\",</span><br /><span style=\"font-size: 8pt;\"> \"text\":\"Im ready for whatever\",</span><br /><span style=\"font-size: 8pt;\"> \"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\/download\\/iphone\\\" rel=\\\"nofollow\\\"\\u003eTwitter for iPhone\\u003c\\/a\\u003e\",</span><br /><span style=\"font-size: 8pt;\"> \"truncated\":false,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_status_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_user_id_str\":null,</span><br /><span style=\"font-size: 8pt;\"> \"in_reply_to_screen_name\":null,</span><br /><span style=\"font-size: 8pt;\"> \"user\":{</span><br /><span style=\"font-size: 8pt;\"> \"id\":1059349532,</span><br /><span style=\"font-size: 8pt;\"> \"id_str\":\"1059349532\",</span><br /><span style=\"font-size: 8pt;\"> \"name\":\"Don Dada\",</span><br /><span style=\"font-size: 8pt;\"> \"screen_name\":\"ImDatNiggaBD\",</span><br /><span style=\"font-size: 8pt;\"> \"location\":\"South Side Of Little Rock\",</span><br /><span style=\"font-size: 8pt;\"> \"url\":null,</span><br /><span style=\"font-size: 8pt;\"> \"description\":\"Weed Smoker (Kush)\",</span><br /><span style=\"font-size: 8pt;\"> \"protected\":false,</span><br /><span style=\"font-size: 8pt;\"> \"followers_count\":109,</span><br /><span style=\"font-size: 8pt;\"> \"friends_count\":110,</span><br /><span style=\"font-size: 8pt;\"> \"listed_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"created_at\":\"Fri Jan 04 02:37:28 +0000 2013\",</span><br /><span style=\"font-size: 8pt;\"> \"favourites_count\":14,</span><br /><span style=\"font-size: 8pt;\"> \"utc_offset\":null,</span><br /><span style=\"font-size: 8pt;\"> \"time_zone\":null,</span><br /><span style=\"font-size: 8pt;\"> \"geo_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"verified\":false,</span><br /><span style=\"font-size: 8pt;\"> \"statuses_count\":1312,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\",</span><br /><span style=\"font-size: 8pt;\"> \"contributors_enabled\":false,</span><br /><span style=\"font-size: 8pt;\"> \"is_translator\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url\":\"http:\\/\\/a0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_image_url_https\":\"https:\\/\\/si0.twimg.com\\/images\\/themes\\/theme1\\/bg.png\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_background_tile\":false,</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url\":\"http:\\/\\/a0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_image_url_https\":\"https:\\/\\/si0.twimg.com\\/profile_images\\/3184813228\\/d6d3a95d902f088f412cf1bd90c126c7_normal.jpeg\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_banner_url\":\"https:\\/\\/si0.twimg.com\\/profile_banners\\/1059349532\\/1359068332\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_link_color\":\"0084B4\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_border_color\":\"C0DEED\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_sidebar_fill_color\":\"DDEEF6\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_text_color\":\"333333\",</span><br /><span style=\"font-size: 8pt;\"> \"profile_use_background_image\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile\":true,</span><br /><span style=\"font-size: 8pt;\"> \"default_profile_image\":false,</span><br /><span style=\"font-size: 8pt;\"> \"following\":null,</span><br /><span style=\"font-size: 8pt;\"> \"follow_request_sent\":null,</span><br /><span style=\"font-size: 8pt;\"> \"notifications\":null},</span><br /><span style=\"font-size: 8pt;\"> \"geo\":null,</span><br /><span style=\"font-size: 8pt;\"> \"coordinates\":null,</span><br /><span style=\"font-size: 8pt;\"> \"place\":null,</span><br /><span style=\"font-size: 8pt;\"> \"contributors\":null,</span><br /><span style=\"font-size: 8pt;\"> \"retweet_count\":0,</span><br /><span style=\"font-size: 8pt;\"> \"entities\":{\"hashtags\":[],</span><br /><span style=\"font-size: 8pt;\"> \"urls\":[],</span><br /><span style=\"font-size: 8pt;\"> \"user_mentions\":[]},</span><br /><span style=\"font-size: 8pt;\"> \"favorited\":false,</span><br /><span style=\"font-size: 8pt;\"> \"retweeted\":false,</span><br /><span style=\"font-size: 8pt;\"> \"lang\":\"en\"<strong>}</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Obviously, not all attributes are relevant to our analysis. In the context of this paper, the only releavant fields in our features are:</strong></span></p>\n",
    "<p><span style=\"color: #0000ff;\"><em><strong>Hashtags, From_User, Create_Time, Location, Mentions</strong></em></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Moreover, the raw text is quite dirty. We need to perform some data cleaning to get proper features.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Since is step is fairly involveda and independet of the analysis here, I keep them in a separate Notebook. </strong></span></p>\n",
    "<blockquote>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>Spark-Twt-PreProcessing.ipynb</strong></span></p>\n",
    "</blockquote>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong> You should be able to follow along as an indepent module.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>The resulting data looks like this:</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Processed-tweet:</strong></p>\n",
    "<p><strong>{</strong>u'Create_time': 1359737884.0,<br /> u'from_id': 87151732,<br /> u'from_user': u'ishiPTI',<br /> u'hashtag': u'thuglife',<br /> u'location': u'loc_lakeshore',<br /> u'mention': u'BushraShekhani',<br /> u'term': u'I am ready for whatever',<br /> u'tweet_id': 297312861586325504<strong>}</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have a small (sort of) and clean dataset to work with, it is time to move on to spark to perform some reall analysis.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We are on to the real coding part. Please note that the Spak code presented here probably violates every single good coding practice, not to mention OOP...The point is to make this notebook as illustrative as possible, you will probably see a lot of redundant code. Feel free to refactor as you wish..</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step Two: Feature Extraction</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We need to turn the raw json data into a feature matrix. There are two keys here: </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>1. Data processing must be extremly efficient since we only have 40 cores and 256G ram.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>2. The resulting matrix must be sparse to facilitate the training step&nbsp;later.</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>These are achieved through the following pipeline. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1\n",
    "\n",
    "## Notebook property setup.\n",
    "## Spark SQL\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, col, lit, monotonically_increasing_id, explode\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "## Spark ML\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover, CountVectorizer, VectorAssembler\n",
    "\n",
    "## Helper\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import os.path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "## Enable inline graphs\n",
    "%matplotlib inline\n",
    "\n",
    "## Display precision for pandas dataframe\n",
    "pd.set_option('precision',10)\n",
    "\n",
    "workdir_1e = \"data/Eng_Json/\" #Parsed json for year 2013\n",
    "#workdir_2b = \"/mnt/2b53fde0-61da-4eeb-a038-9910540ff9ad/Eng_Json/\" #Parsed json for year 2014\n",
    "workdir_4e = \"data/final_parquet\" #Dir to hold input data in parquet format \n",
    "workdir_66 = \"data/Training_data\" #Dir to hold training data\n",
    "workdir_b9 = \"data/Feature_Vector\" #Dir to hold processed Feature vectors\n",
    "# Sample bash code to change folder access.\n",
    "# !chgrp danielshi /mnt/66e695cd-1a0c-4e3b-9a50-55e01b788529/Training_data/\n",
    "# !chmod g+s /mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Feature_Vector/\n",
    "# !setfacl -d -m g::rwx /mnt/b93e71ec-8ddf-4033-bd42-770c05bc68aa/Feature_Vector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Helper function to keep track of the run time of a spark task.\n",
    "def getTime(start):\n",
    "    sec = time.time() - start\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print('Spark operation takes - %d:%02d:%02d which is %d seconds in total' % (h,m,s,sec))\n",
    "    \n",
    "# load json object, if a line is invalid, substitute as an empty dict (which has len() == 0 )\n",
    "def loadJson(d):\n",
    "    try:\n",
    "        js = json.loads(d)\n",
    "    except ValueError as e:\n",
    "        js = {}\n",
    "    except Exception:\n",
    "        js = {}\n",
    "    return js\n",
    "\n",
    "def translating(x):\n",
    "    return x.encode('utf-8').lower().translate(None, string.punctuation)\n",
    "\n",
    "def loc_clean(d):\n",
    "\n",
    "    if d == None or d.strip(' ') == '':\n",
    "        loc_term = \"empty_location\"\n",
    "    else:\n",
    "        loc_term = 'loc_' + \"_\".join(map(translating, d.strip(' ').split(\" \")))\n",
    "        \n",
    "    return loc_term\n",
    "\n",
    "loc_udf = udf(loc_clean, StringType())\n",
    "\n",
    "\n",
    "def hash_clean(d):\n",
    "\n",
    "    if d == None or d.strip(' ') == '':\n",
    "        hashtags = \"empty_hashtag\"\n",
    "    else:\n",
    "        hashtags = d\n",
    "        \n",
    "    return hashtags\n",
    "\n",
    "hash_udf = udf(hash_clean, StringType())\n",
    "\n",
    "\n",
    "def mention_clean(d):\n",
    "\n",
    "    if d == None or d.strip(' ') == '':\n",
    "        mentions = \"empty_mention\"\n",
    "    else:\n",
    "        mentions = d\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "mention_udf = udf(mention_clean, StringType())\n",
    "\n",
    "\n",
    "def clean_term(d):\n",
    "    tags = d['hashtag'].split()\n",
    "    user = d['from_user'].split()\n",
    "    mention = d['mention'].split()  \n",
    "    text = d['term'].encode('ascii', 'ignore')\n",
    "    for ppl in mention:\n",
    "        text = text.replace('@'+ppl, '')\n",
    "    for tag in tags:\n",
    "        text = text.replace('#'+tag, '')\n",
    "        \n",
    "    text = re.sub(r'(https?://\\S+)', '',text).replace(\":\", \"\").lower()\n",
    "\n",
    "\n",
    "    if text == None or text.strip(' ') == '':\n",
    "        terms = \"empty_tweet\"\n",
    "    else:\n",
    "        terms = \" \".join(text.encode('utf-8').translate(None, string.punctuation).strip().split())\n",
    "\n",
    "    updated = {'create_time': d['create_time'],\n",
    "      'from_id': d['from_id'],\n",
    "      'from_user': d['from_user'],\n",
    "      'hashtag': d['hashtag'],\n",
    "      'location': d['location'],\n",
    "      'mention': d['mention'],\n",
    "      'term': terms,\n",
    "      'tweet_id': d['tweet_id']}    \n",
    "    return updated\n",
    "\n",
    "\n",
    "def finalCLeaning(file_obj, output):\n",
    "    data_1 = file_obj.map(loadJson)\n",
    "    cleaned_dat = data_1.map(clean_term)\n",
    "    df_p1 = sqlContext.createDataFrame(cleaned_dat, schema)\n",
    "    df_p1.write.save(workdir_66+output, format=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 18px;\"><strong>Reading Data </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>After preprocessing, tweets are saved as parquet files. We need to load and parse these data into dataframes. Note that, the sc.textFile function's input arg could be either a file or a directory. Spark context will create partitions automatically. Note that the pre-processed data are stored in two directories for I/O balancing.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'create_time': 1360036679.0,\n",
       "  u'from_id': u'132727520',\n",
       "  u'from_user': u'AndroidJunkies',\n",
       "  u'hashtag': u'',\n",
       "  u'location': u'',\n",
       "  u'mention': u'',\n",
       "  u'term': u'Breaking \\u2013 Jelly Bean update for the Verizon Galaxy S3 leaked!: Android 4.1.2, the latest flavor of Jelly Bean o... http://t.co/nPbyaHqS',\n",
       "  u'tweet_id': u'298566098620665858'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full\n",
    "sc =SparkContext()\n",
    "\n",
    "data_Eng = sc.textFile(workdir_1e)\n",
    "data = data_Eng.map(loadJson)\n",
    "# Take a look at the (parsed) first line of our input files\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Turning to dataframe</strong></span></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>A RDD (Resilient Distributed Dataset) is more of a blackbox dataset (available in Spark since 1.0) </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>A dataframe is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case. Therefore, a DataFrame has additional metadata due to its tabular format, which allows Spark to run certain optimizations on the finalized query. (Added since 1.3) </strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>In summary, you are able to write traditional map-reduce type of code on both RDD and Dataframe, but Dataframe also support SQL command and built-in analytical functions. For performance consideration, let's turn our RDD into Dataframes first.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define Dataframe schema.\n",
    "schema = StructType([StructField('create_time', DoubleType(), False),\n",
    "                     StructField('from_id', StringType(), False),\n",
    "                     StructField('from_user', StringType(), False),\n",
    "                     StructField('hashtag', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('mention', StringType(), True),\n",
    "                     StructField('term', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), False)\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Saving dataframe to parquet format for easy loading. NOTE: you will see a lot of I/O code being commented out. These are intermeidte results used to produce other DFs later. We don't need to run them everytime; only run if you want to reproduce the result.</strong></span></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.save(workdir_4e+\"/Eng_DF\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Input is shown in tabular form (Dataframe) below. Note that hashtag field just happend to be null for the first few records. </strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Clean the hashtag, mention, username and location by removing null and un-wanted chars/punctuations. We utilized user-defined functions here. Essentially this is the same concept of apply a custom map operation on a column</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o78.showString.\n: java.lang.IllegalArgumentException\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:449)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:432)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:432)\n\tat org.apache.xbean.asm5.ClassReader.a(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.b(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:262)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:261)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:261)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:796)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:93)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.BaseLimitExec$class.inputRDDs(limit.scala:62)\n\tat org.apache.spark.sql.execution.LocalLimitExec.inputRDDs(limit.scala:97)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a25bc7ae9575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean_stage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clean_loc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m                \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clean_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m                \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clean_mention\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmention_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_stage1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o78.showString.\n: java.lang.IllegalArgumentException\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:449)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:432)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:432)\n\tat org.apache.xbean.asm5.ClassReader.a(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.b(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:262)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:261)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:261)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:796)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:93)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.BaseLimitExec$class.inputRDDs(limit.scala:62)\n\tat org.apache.spark.sql.execution.LocalLimitExec.inputRDDs(limit.scala:97)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n"
     ]
    }
   ],
   "source": [
    "clean_stage1 = df.withColumn(\"clean_loc\", loc_udf(df.location)).\\\n",
    "                withColumn(\"clean_hash\", hash_udf(df.hashtag)).\\\n",
    "                withColumn(\"clean_mention\", mention_udf(df.mention))\n",
    "clean_stage1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_stage1.write.save(workdir_4e+\"/Eng_DF_clean_stage1\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_stage1 = spark.read.parquet(workdir_4e+\"/Eng_DF_clean_stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map back to the old column names\n",
    "Pre_cleansing_df = clean_stage1.select(clean_stage1.create_time, \\\n",
    "                   clean_stage1.from_id, \\\n",
    "                   clean_stage1.from_user, \\\n",
    "                   clean_stage1.tweet_id, \\\n",
    "                   clean_stage1.term, \\\n",
    "                   clean_stage1.clean_loc, \\\n",
    "                   clean_stage1.clean_hash, \\\n",
    "                   clean_stage1.clean_mention)\n",
    "\n",
    "clean_stage2 = Pre_cleansing_df.withColumnRenamed(\"clean_loc\", \"location\").withColumnRenamed(\"clean_hash\", \"hashtag\").withColumnRenamed(\"clean_mention\", \"mention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_stage2.write.json(workdir_66+\"/Eng_DF_clean_stage2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_stage2 = sc.textFile(workdir_66+\"/Eng_DF_clean_stage2/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reframing\n",
    "# Perform cleaning for the terms column. This used to be the most time consuming step during processing; therefore it was separated\n",
    "# from the previous stage. However, after some optimization, right now it takes roughly the same time as the other cleaning steps.\n",
    "# Feel free to combine this with the previous stage if you want.\n",
    "\n",
    "# Note that the convention followed here is more like \"hadoop\" since intermediate steps are saved separately. Although not \n",
    "# necessary, it ensures we have somewhere to fall back on if incuring any problem at some stage. \n",
    "\n",
    "#finalCLeaning(clean_stage2, \"/Staging_final\")\n",
    "Stg_final = spark.read.parquet(workdir_66+\"/Staging_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now our dataframe is nice and clean, the next task is to label the dataset. Remember our criteria is that a tweet is topical if it contains one of our pre-defined hashtag list for a given topic. Note that a tweet could contain multiple hashtags, if one of them is releavant, we would consider the tweet as releavant. Therefore, what we need to do here is to flatten the hashtag list, find an unique list of tweet ids which contains releavant hashtags, and then join it back to the original DF. Obviously, there are more than one way to achieve this. I try to stick with dataframe-only operations here for efficiency.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the hashtag dict from another file.\n",
    "from hashtag_dict import topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access a particular topic\n",
    "#topic_dict['Soccer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Each tweet could contain multiple hashtags, we need to normalize this attribute. This will facilitate the labeling step later.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")\n",
    "hashtags_df = tokenizer.transform(Stg_final)\n",
    "\n",
    "hashtag =  hashtags_df.select(\"tweet_id\",\"create_time\",\"each_hashtag\")\n",
    "hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash_exploded.write.save(workdir_4e+\"/hash_exploded\", format=\"parquet\")\n",
    "hash_exploded = spark.read.parquet(workdir_4e+\"/hash_exploded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>If a hashtag is in the predefined list, we mark the corresponding tweet as topical. Using distinct ops to get a unique list of topical id for a particular topic.  </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Tennis\")\n",
    "print(\"num hastag: \" + len(topic_dict[\"Tennis\"]))\n",
    "tennis_topical_ids = hash_exploded.select(hash_exploded.tweet_id).where(hash_exploded.each_hashtag.isin(topic_dict[\"Tennis\"])).distinct().cache()\n",
    "#print(topic_ids.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong> Now we have a list of tweets ids that are topical for tennis</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis_topical_ids.write.save(workdir_66+\"/tennis_topical_ids\", format=\"parquet\")\n",
    "tennis_topical_ids = spark.read.parquet(workdir_66+\"/tennis_topical_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis_topical_ids.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong> Join the ids back to obtain the full label</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabeledDf(df_topic):\n",
    "    Labeled_topical = df_topic.withColumn(\"topical\", lit(1))\n",
    "    Labled_df = Stg_final.join(Labeled_topical, Stg_final.tweet_id == Labeled_topical.tweet_id, \"left\").\\\n",
    "                                       select(Stg_final.create_time,\\\n",
    "                                              Stg_final.from_id,\\\n",
    "                                              Stg_final.from_user,\\\n",
    "                                              Stg_final.hashtag,\\\n",
    "                                              Stg_final.location,\\\n",
    "                                              Stg_final.mention,\\\n",
    "                                              Stg_final.tweet_id,\\\n",
    "                                              Stg_final.term,\\\n",
    "                                              F.when(Labeled_topical.topical == 1, 1.0).otherwise(0.0).alias(\"label\")).distinct()\n",
    "    \n",
    "    return Labled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labled_df = getLabeledDf(tennis_topical_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labled_df.write.save(workdir_4e+\"/Labled_df\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labled_df = spark.read.parquet(workdir_4e+\"/Labled_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis_labels = Labled_df.select(\"label\",\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis_text = Labled_df.select(\"tweet_id\",\"create_time\",\"from_user\",\"hashtag\",\"location\",\"mention\",\"term\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Saving the full label column for later use.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tennis_labels.write.save(workdir_66+\"/tennis_topical_labels\", format=\"parquet\")\n",
    "tennis_labels = spark.read.parquet(workdir_66+\"/tennis_topical_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tennis_text.write.save(workdir_66+\"/tennis_raw_feature_text\", format=\"parquet\")\n",
    "tennis_text = spark.read.parquet(workdir_66+\"/tennis_raw_feature_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Vectorizing user, hashtag, location, mention, term into feature vectors</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We have an overwhelming number of features; it is essentail to threshold them to avoid overfitting. We use the same threshold as describbed in the paper. Note that the threshold is for DF, not TF.</strong></span></p>\n",
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/featurecount.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>In this section, we vectorize each feature according to the count threshold above. Note that we cannot chain this in a pipeline, as the threshold must be appied to the original dataframe.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Term Feature Threshold. Removing stop wprds first, and only take feature with df count > 50 </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"words\")\n",
    "term_remover = StopWordsRemover(inputCol=term_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "term_cv = CountVectorizer(inputCol=term_remover.getOutputCol(), outputCol=\"term_features\", minDF=50)\n",
    "pipeline_term = Pipeline(stages=[term_tokenizer,term_remover,term_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_term.fit(Labled_df)\n",
    "Feat_term = model.transform(Labled_df).select(\"term_features\", \"tweet_id\")\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feat_term.write.save(workdir_b9+\"/Feature_term\", format=\"parquet\")\n",
    "Feat_term = spark.read.parquet(workdir_b9+\"/Feature_term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check what features passes the threshold.\n",
    "Feat_term.show()\n",
    "#http://stackoverflow.com/questions/32285699/how-to-get-word-details-from-tf-vector-rdd-in-spark-ml-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Hashtag Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"tags\")\n",
    "hashtag_cv = CountVectorizer(inputCol=hashtag_tokenizer.getOutputCol(), outputCol=\"hashtag_features\", minDF=159)\n",
    "pipeline_hashtag = Pipeline(stages=[hashtag_tokenizer,hashtag_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_hashtag.fit(Labled_df)\n",
    "Feat_hashtag = model.transform(Labled_df).select(\"hashtag_features\", col(\"tweet_id\").alias(\"id2\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feat_hashtag.write.save(workdir_b9+\"/Feature_hashtag\", format=\"parquet\")\n",
    "Feat_hashtag = spark.read.parquet(workdir_b9+\"/Feature_hashtag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Mention Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_tokenizer = Tokenizer(inputCol=\"mention\", outputCol=\"mentions\")\n",
    "mention_cv = CountVectorizer(inputCol=mention_tokenizer.getOutputCol(), outputCol=\"mention_features\", minDF=159)\n",
    "pipeline_mention = Pipeline(stages=[mention_tokenizer,mention_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_mention.fit(Labled_df)\n",
    "Feat_mention = model.transform(Labled_df).select(\"mention_features\", col(\"tweet_id\").alias(\"id3\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feat_mention.write.save(workdir_b9+\"/Feature_mention\", format=\"parquet\")\n",
    "Feat_mention = spark.read.parquet(workdir_b9+\"/Feature_mention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>User Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tokenizer = Tokenizer(inputCol=\"from_user\", outputCol=\"users\")\n",
    "user_cv = CountVectorizer(inputCol=user_tokenizer.getOutputCol(), outputCol=\"user_features\", minDF=159)\n",
    "pipeline_user = Pipeline(stages=[user_tokenizer,user_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_user.fit(Labled_df)\n",
    "Feat_user = model.transform(Labled_df).select(\"user_features\", col(\"tweet_id\").alias(\"id4\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feat_user.write.save(workdir_b9+\"/Feature_user\", format=\"parquet\")\n",
    "Feat_user = spark.read.parquet(workdir_b9+\"/Feature_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Location Feature Threshold. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_tokenizer = Tokenizer(inputCol=\"location\", outputCol=\"locs\")\n",
    "loc_cv = CountVectorizer(inputCol=loc_tokenizer.getOutputCol(), outputCol=\"loc_features\", minDF=50)\n",
    "pipeline_loc = Pipeline(stages=[loc_tokenizer,loc_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "model = pipeline_loc.fit(Labled_df)\n",
    "Feat_loc = model.transform(Labled_df).select(\"loc_features\", \"hashtag\", \"create_time\", col(\"tweet_id\").alias(\"id5\"))\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feat_loc.write.save(workdir_b9+\"/Feature_loc\", format=\"parquet\")\n",
    "Feat_loc = spark.read.parquet(workdir_b9+\"/Feature_loc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Joining all feature DFs above into one. </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feat_1 = Feat_term.join(Feat_hashtag,\\\n",
    "                         Feat_term.tweet_id == Feat_hashtag.id2,\\\n",
    "                         \"inner\").select(Feat_term.term_features,\\\n",
    "                                         Feat_hashtag.hashtag_features,\\\n",
    "                                         Feat_hashtag.id2)\n",
    "Feat_2 = Feat_1.join(Feat_mention,\\\n",
    "                     Feat_1.id2 == Feat_mention.id3,\\\n",
    "                     \"inner\").select(Feat_1.term_features,\\\n",
    "                                     Feat_1.hashtag_features,\\\n",
    "                                     Feat_mention.mention_features,\\\n",
    "                                     Feat_mention.id3)\n",
    "Feat_3 = Feat_2.join(Feat_user,\\\n",
    "                     Feat_2.id3 == Feat_user.id4,\\\n",
    "                     \"inner\").select(Feat_2.term_features,\\\n",
    "                                     Feat_2.hashtag_features,\\\n",
    "                                     Feat_2.mention_features,\\\n",
    "                                     Feat_user.user_features,\\\n",
    "                                     Feat_user.id4)\n",
    "Feat_all = Feat_3.join(Feat_loc,\\\n",
    "                     Feat_3.id4 == Feat_loc.id5,\\\n",
    "                     \"inner\").select(Feat_3.term_features,\\\n",
    "                                     Feat_3.hashtag_features,\\\n",
    "                                     Feat_3.mention_features,\\\n",
    "                                     Feat_3.user_features,\\\n",
    "                                     Feat_loc.loc_features,\\\n",
    "                                     Feat_loc.create_time,\\\n",
    "                                     Feat_loc.hashtag,\\\n",
    "                                     Feat_loc.id5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feat_all.write.save(workdir_66+\"/Feature_agg\", format=\"parquet\")\n",
    "Features_vect = spark.read.parquet(workdir_66+\"/Feature_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do this is to chain everything into a pipeline.\n",
    "''''\n",
    "term_tokenizer = Tokenizer(inputCol=\"term\", outputCol=\"words\")\n",
    "term_remover = StopWordsRemover(inputCol=term_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "term_cv = CountVectorizer(inputCol=term_remover.getOutputCol(), outputCol=\"term_features\", minDF=50)\n",
    "\n",
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"tags\")\n",
    "hashtag_cv = CountVectorizer(inputCol=hashtag_tokenizer.getOutputCol(), outputCol=\"hashtag_features\", minDF=159)\n",
    "\n",
    "mention_tokenizer = Tokenizer(inputCol=\"mention\", outputCol=\"mentions\")\n",
    "mention_cv = CountVectorizer(inputCol=mention_tokenizer.getOutputCol(), outputCol=\"mention_features\", minDF=159)\n",
    "\n",
    "user_tokenizer = Tokenizer(inputCol=\"from_user\", outputCol=\"users\")\n",
    "user_cv = CountVectorizer(inputCol=user_tokenizer.getOutputCol(), outputCol=\"user_features\", minDF=159)\n",
    "\n",
    "loc_tokenizer = Tokenizer(inputCol=\"location\", outputCol=\"locs\")\n",
    "loc_cv = CountVectorizer(inputCol=loc_tokenizer.getOutputCol(), outputCol=\"loc_features\", minDF=50)\n",
    "\n",
    "pipeline = Pipeline(stages=[term_tokenizer,term_remover,term_cv,hashtag_tokenizer,hashtag_cv,mention_tokenizer, \\\n",
    "                            mention_cv,user_tokenizer, user_cv, loc_tokenizer, loc_cv])\n",
    "\n",
    "loading = time.time()\n",
    "\n",
    "model = pipeline.fit(clean_data)\n",
    "Input = model.transform(clean_data)\n",
    "\n",
    "getTime(loading)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Lastly, we join the data frame with the topical labels we had earlier. Now we have both different list of feature vectors and the coresponding labels.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labled_Feat = Features_vect.join(tennis_labels,\\\n",
    "                                 Features_vect.id5 == tennis_labels.tweet_id,\\\n",
    "                                 \"inner\").select(Features_vect.term_features,\\\n",
    "                                                 Features_vect.hashtag_features,\\\n",
    "                                                 Features_vect.mention_features,\\\n",
    "                                                 Features_vect.user_features,\\\n",
    "                                                 Features_vect.loc_features,\\\n",
    "                                                 Features_vect.create_time,\\\n",
    "                                                 Features_vect.hashtag,\\\n",
    "                                                 tennis_labels.label,\\\n",
    "                                                 tennis_labels.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labled_Feat.write.save(workdir_b9+\"/Features_with_label\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labled_Feat = spark.read.parquet(workdir_b9+\"/Features_with_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labled_Feat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>At this point, each feature vector is still in its separate column. We need to combine them into one feature matrix. However, before we do that, let's split our dataset first. The reason for this is that Apache parquet is not very good at handling sparse data. Saving such data will likely run into memory error. We hold off the combining step for later.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Temporal Split</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have our feature matrix, it is time to estabulish the training, validation and test set for training the classifier</strong></span></p>\n",
    "<p><span style=\"font-size: 13.3333px;\"><strong>To ensure our classifier generalize to a wide range of features and not simply remeber the past hashtag, we will perform a temporal split to exclude training hashtags in validation and test.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/Capture.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the the 50%-10%-40% split ratio is not mandetory. I suggest to examine the dataframe for different topic \n",
    "# and make your decision on the go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Hashtag Birthday</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Hashtag birthday indicates the first timestamp that a particular hashtag appears in the tweet corpus between year 2013 and 2014. We determine this by find the minimum \"create time\" for each hashtag </strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_birthday = hash_exploded.join(tennis_labels,\\\n",
    "                                 hash_exploded.tweet_id == tennis_labels.tweet_id,\\\n",
    "                                 \"inner\").select(hash_exploded.create_time,\\\n",
    "                                                 hash_exploded.each_hashtag,\\\n",
    "                                                 hash_exploded.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find out the \"birthday\", or the earliest appearing time of each hashtag. \n",
    "## (add an extra column of 1 to mark as topical, will be used in a join later)\n",
    "Ordered_Hashtag_set = df_birthday.\\\n",
    "                      groupby(\"each_hashtag\").\\\n",
    "                      agg({\"create_time\": \"min\"}).\\\n",
    "                      orderBy('min(create_time)', ascending=True).\\\n",
    "                      withColumnRenamed(\"min(create_time)\", \"birthday\").\\\n",
    "                      where(df_birthday.each_hashtag.isin(topic_dict[\"Tennis\"])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check\n",
    "Ordered_Hashtag_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_span = Ordered_Hashtag_set.count()\n",
    "\n",
    "# Get id of the corresponding time split (75% and 85%). Again, you need to look at how many data we have for train-valid-test. \n",
    "# The split ratio should be determined on a case by case basis.\n",
    "\n",
    "train_val_split_Ht = np.floor(np.multiply(time_span, 0.75)).astype(int)\n",
    "val_test_split_Ht =  np.floor(np.multiply(time_span, 0.85)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to Pandas for random row access.\n",
    "pd_Ordered_Hashtag_set = Ordered_Hashtag_set.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the timestamp of the cutoff point. Will be used later to split Dataframe.\n",
    "train_val_time = pd_Ordered_Hashtag_set.iloc[train_val_split_Ht]['birthday']\n",
    "val_test_time = pd_Ordered_Hashtag_set.iloc[val_test_split_Ht]['birthday']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashtags = pd_Ordered_Hashtag_set[:train_val_split_Ht][\"each_hashtag\"].tolist()\n",
    "train_hashtags = [x.encode('utf-8') for x in train_hashtags]\n",
    "\n",
    "val_hashtags = pd_Ordered_Hashtag_set[train_val_split_Ht:val_test_split_Ht][\"each_hashtag\"].tolist()\n",
    "val_hashtags = [x.encode('utf-8') for x in val_hashtags]\n",
    "\n",
    "test_hashtags = pd_Ordered_Hashtag_set[val_test_split_Ht:][\"each_hashtag\"].tolist()\n",
    "test_hashtags = [x.encode('utf-8') for x in test_hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_val_time)\n",
    "print(val_test_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have identified the hashtags to be used in training, validation and test set, we can proceed to split our dataframe.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://github.com/demoonism/TwitterSensor/blob/master/Screenshot/remove_twit.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Train-Valid-Test split</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>All data points happended before the train/valid split time are labeled as training data</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set = Labled_Feat.select(Labled_Feat.create_time,\\\n",
    "                              Labled_Feat.tweet_id,\\\n",
    "                              Labled_Feat.term_features,\\\n",
    "                              Labled_Feat.hashtag_features,\\\n",
    "                              Labled_Feat.mention_features,\\\n",
    "                              Labled_Feat.user_features,\\\n",
    "                              Labled_Feat.loc_features,\\\n",
    "                              Labled_Feat.hashtag,\\\n",
    "                              Labled_Feat.label).where(col(\"create_time\") <= train_val_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading = time.time()\n",
    "\n",
    "tr_pos_sample = Training_set.where(col(\"label\") == 1.0).count()\n",
    "\n",
    "getTime(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to see how many positive data we have in training by spliting on the current ratio. We need to ensure \n",
    "# that we have enough data points to train on. \n",
    "tr_pos_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>All data points happended between the train/valid & valid/test split time are labeled as validation data. We also need to remove any hashtag that appeared in the training set from the validation set.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_set = Labled_Feat.select(Labled_Feat.create_time,\\\n",
    "                                Labled_Feat.tweet_id,\\\n",
    "                                Labled_Feat.term_features,\\\n",
    "                                Labled_Feat.hashtag_features,\\\n",
    "                                Labled_Feat.mention_features,\\\n",
    "                                Labled_Feat.user_features,\\\n",
    "                                Labled_Feat.loc_features,\\\n",
    "                                Labled_Feat.hashtag,\\\n",
    "                                Labled_Feat.label).where((col(\"create_time\") > train_val_time) & (col(\"create_time\") <= val_test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_tokenizer = Tokenizer(inputCol=\"hashtag\", outputCol=\"each_hashtag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hashtags_df = hashtag_tokenizer.transform(Validation_set)\n",
    "hashtag =  val_hashtags_df.select(\"tweet_id\",\"each_hashtag\")\n",
    "val_hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hash_exploded.write.save(workdir_66+\"/valid_hash_exploded\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hash_exploded = spark.read.parquet(workdir_66+\"/valid_hash_exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invalid_Val_ids = val_hash_exploded.select(\"tweet_id\").\\\n",
    "                                           where(val_hash_exploded.each_hashtag.isin(train_hashtags)).\\\n",
    "                                           distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invalid_Val_ids_list = Invalid_Val_ids.distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If a hashtag appeared in training set, discard this record\n",
    "\n",
    "Validation_set_no_train = Validation_set.where(Validation_set.tweet_id.isin(Invalid_Val_ids_list) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_sample = Validation_set_no_train.where(col(\"label\") == 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>All data points happended after the valid/test split time are labeled as test data. We also need to remove any records that have hashtag that appeared in training&validation set.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_set = Labled_Feat.select(Labled_Feat.create_time,\\\n",
    "                            Labled_Feat.tweet_id,\\\n",
    "                            Labled_Feat.term_features,\\\n",
    "                            Labled_Feat.hashtag_features,\\\n",
    "                            Labled_Feat.mention_features,\\\n",
    "                            Labled_Feat.user_features,\\\n",
    "                            Labled_Feat.loc_features,\\\n",
    "                            Labled_Feat.hashtag,\\\n",
    "                            Labled_Feat.label).where(col(\"create_time\") > val_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hashtags_df = hashtag_tokenizer.transform(Test_set)\n",
    "hashtag = test_hashtags_df.select(\"tweet_id\",\"each_hashtag\")\n",
    "test_hash_exploded = hashtag.withColumn('each_hashtag', explode('each_hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hash_exploded.write.save(workdir_66+\"/te_hash_exploded\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hash_exploded = spark.read.parquet(workdir_66+\"/te_hash_exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invalid_Test_ids = test_hash_exploded.select(\"tweet_id\").where((test_hash_exploded.each_hashtag.isin(train_hashtags)) | (test_hash_exploded.each_hashtag.isin(val_hashtags))).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invalid_Test_ids_list = Invalid_Test_ids.distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_set_no_train_no_vaild = Test_set.where(Test_set.tweet_id.isin(Invalid_Test_ids_list) == False).\\\n",
    "                                         dropDuplicates(['term_features', 'hashtag_features', 'mention_features', 'user_features', 'term_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling data to balance label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong> Down-sampling negative data to balance out the training data</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate pos and neg training samples to form the final training set.\n",
    "Training_set_balanced = Training_set.sampleBy(\"label\", fractions={0.0: 0.001, 1.0: 1}, seed=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set_balanced.write.save(workdir_b9+\"/Train_balanced\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_set_balanced = spark.read.parquet(workdir_b9+\"/Train_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_set_no_train_balanced = Validation_set_no_train.sampleBy(\"label\", fractions={0.0: 0.0001, 1.0: 1}, seed=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_set_no_train_balanced.write.save(workdir_b9+\"/Validation_balanced\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_set_no_train_balanced = spark.read.parquet(workdir_b9+\"/Test_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_set_no_train_no_vaild.write.save(workdir_b9+\"/Test_balanced\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Now we have a much smaller dataset to work with, it is time to go ahead and concatenate the feature vectors to obtain a single feature matrix. Note that we still keep the tweet id in the output because we want to keep a mapping to the original tweet for manual examination</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Assembling(ds):\n",
    "    assembler = VectorAssembler(inputCols = [\"term_features\",\"hashtag_features\",\"mention_features\",\"user_features\",\"loc_features\"], outputCol=\"features\")\n",
    "    assembled_dataset = assembler.transform(ds).\\\n",
    "                    select(\"tweet_id\",\"create_time\",\"features\", \"label\")\n",
    "    return assembled_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tr_Features = Assembling(Training_set_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_Features =  Assembling(Validation_set_no_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te_Features =  Assembling(Test_set_no_train_no_vaild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = Tr_Features.withColumn(\"type\", lit(\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid = Val_Features.withColumn(\"type\", lit(\"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = Train.union(Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as libSVM format.\n",
    "#MLUtils.saveAsLibSVMFile(combined.select(\"features\").rdd, workdir_b9+\"/combined_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.select(\"features\",\"label\",\"type\").write.save(workdir_b9+\"/combined_dataset\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = spark.read.parquet(workdir_b9+\"/combined_dataset\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #000080;\"><strong><span style=\"font-family: georgia, palatino, serif; font-size: 24pt;\">Step Three: Training Classifier</span></strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>To train an effective classifier, we need to follow two steps: 1. Feature selection 2. Parameter tunning. Here we will be using Chi-sqaure as our feature selection method and tunning L2 penalty and epoch accordingly.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Wrap feature selector into a pipeline, use grid search to determine the optimal number of features. Chi-Square is a similar feature selection technique as mutual information. Utilizing this as its spark-built-in.</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "selector = ChiSqSelector(featuresCol=\"features\",\n",
    "                         outputCol=\"Features_matrix\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color: #000080;\"><strong>Train logistic regression and Hyper Parameter Tunning</strong></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>We are tunning two hyperparameters for the logistic regression, namly number of features and L2 penalty</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#Spark does not have a built-in evaluator class for average precision or P@K. We can extend the base CrossEvaluator class\n",
    "#to implement our own evaluator. Check LogLossEvaluator.py to find more details.\n",
    "from LogLossEvaluator import BinaryRankingEvaluator\n",
    "\n",
    "blor = LogisticRegression(featuresCol='Features_matrix', labelCol='label')\n",
    "\n",
    "TrainingPipeline = Pipeline(stages=[selector,blor])\n",
    "\n",
    "Ranker = BinaryRankingEvaluator(metric = \"AP\")\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 13.3333px;\"><strong>Extending the crossValidator class to use our custom  evaluator (P@k and MAP). The Evaluator code can be found in LogLossEvaluator.py</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from LogLossEvaluator import BinaryRankingEvaluator\n",
    "from CrossValidator import CrossValidatorVerbose\n",
    "\n",
    "result = []\n",
    "Ranker = BinaryRankingEvaluator(metric = \"AP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numTopFeatures: number of feature to use, last one = all features.\n",
    "# regParam: L2 Penalty\n",
    "# maxIter: epoch\n",
    "\n",
    "paramGrid = ParamGridBuilder().\\\n",
    "    addGrid(selector.numTopFeatures, [100, 1000, 10000, 100000]).\\\n",
    "    addGrid(blor.regParam, [0.01, 0.1, 1, 10]).\\\n",
    "    addGrid(blor.maxIter, [2, 20, 100, 200, 500]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvImplicit = CrossValidatorVerbose(estimator=TrainingPipeline, numFolds=5, estimatorParamMaps=paramGrid,evaluator=Ranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cvImplicit.fit(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.save(workdir_b9+\"/myModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvModel.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a,b in zip(paramGrid, cvModel.avgMetrics):\n",
    "    print(b,a)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.getParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = cvModel.bestModel.transform(Val_Features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pred = cvModel.bestModel.transform(Te_Features)\n",
    "metric = Ranker.evaluate(pr)\n",
    "print(\"Test AP: \", metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.select('probability', 'rawPrediction', 'prediction', 'label').where(col(\"label\") == 1.0).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "neg_slicer = VectorSlicer(inputCol=\"probability\", outputCol=\"0_prob\", indices=[0])\n",
    "\n",
    "pos_slicer = VectorSlicer(inputCol=\"probability\", outputCol=\"1_prob\", indices=[1])\n",
    "\n",
    "\n",
    "output_stg1 = neg_slicer.transform(pr)\n",
    "output = pos_slicer.transform(output_stg1)\n",
    "\n",
    "\n",
    "Ranked_prediction  = output.select(\"label\",\"prediction\",\"term\",\"hashtag\",\"from_user\").sort(col(\"1_prob\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pred = out.join(tennis_text, \"tweet_id\").select(\"label\",\"prediction\",\"term\",\"hashtag\",\"from_user\")\\\n",
    "                                             .sort(col(\"1_prob\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pred.show(100, truncate = False)\n",
    "# Validation_set_no_train.select(\"hashtag\").where(col(\"label\") == 1.0).show(100, truncate =  False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
